{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeK0zmtXMjEM",
    "outputId": "c4203be5-bb7e-4ba0-8d5f-ed22b7e85898"
   },
   "outputs": [],
   "source": [
    "!pip install -q opencv-python\n",
    "!pip install -q gradio\n",
    "!pip install -q scipy\n",
    "!pip install -q dlib\n",
    "!pip install -q gradio opencv-python pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HKjk1Ku9MvUO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pygame\n",
    "import dlib\n",
    "from gradio import Image\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WisHggtxNK33",
    "outputId": "fbe72668-2a9c-4170-9eb9-6747d669f5ef"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "apBQGQBFNU0F"
   },
   "outputs": [],
   "source": [
    "sound_path= \"alarm1.mp3\"    #\"/content/drive/MyDrive/preview.mp3\"\n",
    "def play_sound():\n",
    "    \"\"\"Plays a sound using pygame.\"\"\"\n",
    "    pygame.mixer.init()\n",
    "    sound = pygame.mixer.Sound(sound_path)\n",
    "    sound.play()\n",
    "    return \"Sound played!\"  # Return a message to display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# from google.colab.patches import cv2_imshow\n",
    "def outline_face(frame):\n",
    "    # Load Haar cascade classifiers\n",
    "    face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    eye_cascade = cv2.CascadeClassifier(\"haarcascade_eye.xml\")\n",
    "    mouth_cascade = cv2.CascadeClassifier(\"haarcascade_mcs_mouth.xml\")\n",
    "    #smile_cascade = cv2.CascadeClassifier(\"haarcascade_smile.xml\")\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Draw rectangles around faces and eyes\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 255, 0), 2)  # Face rectangle (blue)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = frame[y:y + h, x:x + w]\n",
    "        muts = mouth_cascade.detectMultiScale(roi_gray)\n",
    "        #smiles = smile_cascade.detectMultiScale(roi_gray)\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (mx, my, mw, mh) in muts:\n",
    "            if mw > 0 and mh > 0:\n",
    "                cv2.rectangle(roi_color, (mx, my), (mx + mw, my + mh), (0, 255, 0), 2)   \n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "                cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)  # Eye rectangle (green)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_lighting(frame, use_clahe=False, clip_limit=2.0, tile_grid_size=(8, 8), brightness_threshold=100):\n",
    "\n",
    "    # Convert the frame to YCrCb color space\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    # Compute average brightness (mean of the Y channel)\n",
    "    avg_brightness = np.mean(ycrcb[:, :, 0])\n",
    "    \n",
    "    if avg_brightness < brightness_threshold:\n",
    "        # Apply enhancement only if brightness is below the threshold\n",
    "        if use_clahe:\n",
    "            # Apply CLAHE on the Y channel\n",
    "            clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "            ycrcb[:, :, 0] = clahe.apply(ycrcb[:, :, 0])\n",
    "        else:\n",
    "            # Apply standard histogram equalization on the Y channel\n",
    "            ycrcb[:, :, 0] = cv2.equalizeHist(ycrcb[:, :, 0])\n",
    "    \n",
    "    # Convert back to BGR color space\n",
    "    enhanced_frame = cv2.cvtColor(ycrcb, cv2.COLOR_YCrCb2BGR)\n",
    "    return enhanced_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ear(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to encapsulate drowsiness detection logic\n",
    "class DrowsinessDetector:\n",
    "    def __init__(self, ear_threshold=0.22, consec_frames=20, shape_predictor_path=\"shape_predictor_68_face_landmarks.dat\"):\n",
    "        self.ear_threshold = ear_threshold\n",
    "        self.consec_frames = consec_frames\n",
    "        self.counter = 0\n",
    "        self.alarm_on = False\n",
    "\n",
    "        # Load dlib's models\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "\n",
    "        # Define eye landmarks\n",
    "        self.LEFT_EYE_POINTS = list(range(42, 48))\n",
    "        self.RIGHT_EYE_POINTS = list(range(36, 42))\n",
    "\n",
    "    def detect_drowsiness(self, frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.detector(gray)\n",
    "        \n",
    "        for face in faces:\n",
    "            # Get landmarks and extract eye regions\n",
    "            shape = self.predictor(gray, face)\n",
    "            landmarks = np.array([[p.x, p.y] for p in shape.parts()])\n",
    "            \n",
    "            # Extract left and right eye coordinates\n",
    "            try:\n",
    "                left_eye = landmarks[self.LEFT_EYE_POINTS]\n",
    "                right_eye = landmarks[self.RIGHT_EYE_POINTS]\n",
    "            except IndexError:\n",
    "                # Handle case where eyes are not detected correctly\n",
    "                print(\"Error: Eye landmarks not detected properly.\")\n",
    "                return frame  # Return the frame without processing further\n",
    "\n",
    "            # Compute EAR (Eye Aspect Ratio) for both eyes\n",
    "            left_ear = compute_ear(left_eye) if left_eye is not None else 0.0\n",
    "            right_ear = compute_ear(right_eye) if right_eye is not None else 0.0\n",
    "    \n",
    "            # Handle case where either left or right EAR is missing\n",
    "            if left_ear == 0.0 or right_ear == 0.0:\n",
    "                print(\"Warning: One or both eyes are not visible.\")\n",
    "                ear = 0.0  # Set EAR to a default value when eyes are not visible\n",
    "            else:\n",
    "                # Compute average EAR\n",
    "                ear = (left_ear + right_ear) / 2.0\n",
    "\n",
    "            # Check EAR threshold for drowsiness detection\n",
    "            if ear < self.ear_threshold:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.consec_frames and not self.alarm_on:\n",
    "                    print(\"Drowsiness detected!\")\n",
    "                    self.alarm_on = True\n",
    "                    self.trigger_alarm()\n",
    "            else:\n",
    "                self.counter = 0\n",
    "                self.alarm_on = False\n",
    "\n",
    "            # Visual feedback: Drawing eye landmarks and EAR value on the frame\n",
    "            if left_eye is not None:\n",
    "                cv2.polylines(frame, [left_eye], True, (0, 255, 0), 1)\n",
    "            if right_eye is not None:\n",
    "                cv2.polylines(frame, [right_eye], True, (0, 255, 0), 1)\n",
    "                cv2.putText(frame, f\"Alertness: {ear:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "        return frame\n",
    "\n",
    "    def trigger_alarm(self):\n",
    "        play_sound()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep_detections(frame, detector):\n",
    "    return detector.detect_drowsiness(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_face(frame,resize_dim=(600, 600)):\n",
    "    \"\"\"\n",
    "    Crop and centralize the face in the frame.\n",
    "    \"\"\"\n",
    "    face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    image_padding= 20\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    if len(faces) > 0:\n",
    "        # Take the first detected face\n",
    "        x, y, w, h = faces[0]\n",
    "        # Ensure coordinates are within bounds\n",
    "        x = max(0, x-image_padding)\n",
    "        y = max(0, y-image_padding)\n",
    "        w = min(w+image_padding, frame.shape[1] - x)\n",
    "        h = min(h+image_padding, frame.shape[0] - y)\n",
    "        # Crop the face\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        # Resize the face to the specified dimensions\n",
    "        central_frame = cv2.resize(face, resize_dim)\n",
    "        return central_frame\n",
    "    else:\n",
    "        # Return the original frame if no face is detected\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_Enhancement(frame):\n",
    "    frame= improve_lighting(frame,use_clahe=True)\n",
    "    frame= centralize_face(frame)\n",
    "    return (frame) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9QbjVAFlp0H2"
   },
   "outputs": [],
   "source": [
    "def facedection_cv2(frame, facedection):\n",
    "    frame = frame.copy()\n",
    "    if facedection == \"Face Detection\":\n",
    "        frame = outline_face(frame)\n",
    "    elif facedection == \"Sleep Detection\":\n",
    "        frame = sleep_detections(frame,detector)\n",
    "    elif facedection == \"Image Enhancement\":\n",
    "        frame = Image_Enhancement(frame)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "aBadTTG0N2qo",
    "outputId": "981c6693-5f24-49b6-8a74-79c98cb342cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://f5cd3d1adec8f93d17.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f5cd3d1adec8f93d17.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drowsiness detected!\n",
      "Warning: One or both eyes are not visible.\n",
      "Drowsiness detected!\n",
      "Warning: One or both eyes are not visible.\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n",
      "Drowsiness detected!\n"
     ]
    }
   ],
   "source": [
    "image = gr.Image(value=\"logo.jpeg\", width=150, height=150, show_label=False)\n",
    "detector = DrowsinessDetector()\n",
    "with gr.Blocks(\n",
    "    title=\"Driver Drowsiness App\",\n",
    "    css=\"\"\"\n",
    "    .dialog-box {border: 1px solid #ccc; border-radius: 8px; padding: 20px; background-color:white;} \n",
    "    .header-box { border: 1px solid #ccc; background-color: white; padding: 10px; text-align: center;} \n",
    "    \"\"\"\n",
    ") as demo:\n",
    "    with gr.Group(elem_classes=\"dialog-box\"):\n",
    "        with gr.Row(elem_classes=\"header-box\"):\n",
    "            gr.Markdown(\"\"\"  \n",
    "            # Group 9 Driver Drowsiness\n",
    "            ## Aditya, Ranjeet, and Rajesh  \n",
    "            **Powered by Computer Vision**\n",
    "            \"\"\")\n",
    "            image.render()\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            facedection = gr.Dropdown(\n",
    "                choices=[\"Face Detection\", \"Sleep Detection\", \"Image Enhancement\"],\n",
    "                value=\"Face Detection\",\n",
    "                label=\"Select Detection Mode\"\n",
    "            )\n",
    "            input_img = gr.Image(sources=[\"webcam\"], type=\"numpy\", label=\"Input Image\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_img = gr.Image(streaming=True, label=\"Output Image\",show_label=False)\n",
    "        \n",
    "        # Connect input to processing\n",
    "        input_img.stream(facedection_cv2, [input_img, facedection], [output_img],\n",
    "                         time_limit=30, stream_every=0.1, concurrency_limit=30)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
